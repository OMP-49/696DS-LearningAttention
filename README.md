# 696DS-LearningAttention

## Related Work
* [Pay Attention to MLPs](https://arxiv.org/abs/2105.08050)
* [Deep Sets](https://arxiv.org/abs/1703.06114)
* [Low-Rank Bottleneck in Multi-head Attention Models](https://proceedings.mlr.press/v119/bhojanapalli20a/bhojanapalli20a.pdf)
* [Are Transformers Universal Approximators of Sequence-to-Sequence Functions?](https://arxiv.org/pdf/1912.10077.pdf)
* [MLP-Mixer: An all-MLP Architecture for Vision](https://arxiv.org/pdf/2105.01601.pdf)

## Contributors
- Hon Tik Tse (htse@umass.edu)
- Manan Talwar (mtalwar@umass.edu)
- Om Prajapath (oprajapath@umass.edu)
